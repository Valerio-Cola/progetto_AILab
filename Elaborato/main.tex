\documentclass{article}
\usepackage[utf8]{inputenc}
% Language setting
\usepackage[italian]{babel}

% Set page size and margins
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful package
\usepackage{float} 
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\usepackage{fancyhdr}
\usepackage{lipsum} 

\pagestyle{fancy}
\fancyhf{} 

\addbibresource{sample.bib}
\lhead{\textbf{RACER – Raspberry \& Arduino Car for Environmental Recognition}}
\fancyfoot[C]{\thepage}


\begin{document}

\begin{figure}
    \includegraphics[width=0.45\linewidth]{img/logo.png}
\end{figure}

\title{
  \textbf{RACER} \\
  \Large\textbf{Raspberry \& Arduino Car for Environmental Recognition} \\
  \vspace{1cm}
  \large{Progetto per il corso di AiLab Computer Vision - A.A. 2024/2025} \\
  \vspace{0.5cm}
  \large{Docente: Daniele Pannone} \\
  \vspace{0.5cm}
  \large \today\\[24pt]
    
  \large{Cola Valerio \texttt{cola.2060062@studenti.uniroma1.it}}\\
  \large{Cerboni Federico \texttt{cerboni.2006100@studenti.uniroma1.it}} 
}

%\author{Cola Valerio \\ cola.2060062@studenti.uniroma1.it \and Cerboni Federico \\ cerboni.206100@studenti.uniroma1.it}
\date{}
\maketitle

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{img/racer.png}
\end{figure}


\newpage    

\tableofcontents

\newpage    

\section{Introduzione}
    Il progetto RACER si incentra sullo sviluppo da zero di una macchina a guida autonoma grazie alla combinazione del Microcontrollore \cite{arduinomega}
    Arduino Mega 2560 R3 (prodotto dalla Elegoo), il Microcomputer \cite{pizero} Raspberry Pi Zero 2W e il Modello Convoluzionale \cite{yolo} YoloV5s. 
    Sebbene non si tratti di una classica RC-Car, l'idea principale è sostituire il controllo umano e il Radiocomando con un sistema basato su una rete neurale e sulla computer vision tramite OpenCV. Questo sistema è in grado di osservare l'ambiente, rilevare enti e segnali stradali e analizzare le corsie per correggere automaticamente la direzione del veicolo.

    Un primo approccio ha portato il team ad utilizzare un ESP32-S3-N16R8 in combinazione con una camera OV5560. Purtroppo la scarsa potenza di calcolo e la qualità costruttiva ci hanno convinti a sostituirlo con un Raspberry: l’antenna Wi-Fi era troppo debole, i contatti della camera talmente sensibili da compromettere il funzionamento al minimo movimento, e il corpo macchina della OV5560 si surriscaldava rapidamente, peggiorando ulteriormente le prestazioni.

    Il progetto completo è disponibile su GitHub al seguente \href{https://github.com/Valerio-Cola/RACER}{link}.

\section{Progettazione del Veicolo}

\subsection{Arduino - Il cuore}

Alla base di ogni autovettura c'è sempre un motore, per RACER sono state utilizzati 4 piccoli motori DC a bassa tensione (9v) che possono essere controllati con i pin PWM di Arduino. In particolare la struttura è 4WD (Four Wheel Drive), ovvero a trazione integrale dove ogni ruota è motrice e può essere impostata autonomamente.

A disposizione avevamo un solo L293D (dual H-Bridge motor driver) per il controllo di direzione e velocità, sono quindi stati collegati in serie i 2 motori di destra e di sinistra. Ciò semplifica anche la gestione della svolta poichè per sterzare è necessario che i due motori di ambi i lati abbiano velocità equivalenti. I comandi che verranno gestiti sono Start, Stop, Sterzata a destra e sinistra e variazione di velocità a due livelli.
è stato inserito anche uno schermo LCD 16x2 che funge da debugger per comprendere in tempo reale cosa sta facendo l'Arduino.

Il "telaio" è in resina con ruote in gomma e plastica.



\begin{figure}[h!]
\centering
\includegraphics[width=0.65\linewidth]{img/Schema_Motori_Arduino.png}
\caption{Circuito completo dei motori.}
\end{figure}


\subsection{Raspberry Pi Z2W + PiCamera V2 - Gli occhi della macchina}

 Per catturare i dati ambientali abbiamo utilizzato un Raspberry Pi Z2W, dotato di processore Quad-core Cortex-A53 a 1.0 GHz a 64bit e 512MB LPDDR2 di RAM, in combinazione con una \cite{picamera} PiCamera V2 che monta un sensore Sony IMX219 da 8 Megapixel. Entrambi i componenti sono stati montati in un case apposito della Raspberry mostrato nella figura in prima pagina e montato nella parte anteriore del veicolo.

Questo modulo si occuperà di:
\begin{itemize}
\item Catturare un flusso video 720x1280 e streammarlo su una rete locale (Hotspot Cellulare). Lo streaming sarà in contemporanea catturato mediante OpenCV su un PC che sta eseguendo il programma principale di analisi ambientale.
\item Gestire connessioni TCP con un terminale per lo scambio di messaggi.
\item Inoltro dei comandi ad Arduino.
\end{itemize}

\subsection{Comunicazione tra i moduli}
\begin{enumerate}
\item PC collegato alla stessa rete del Raspberry cattura con OpenCV lo streaming video.
\item  Con la libreria Socket invierà i comandi/messaggi utilizzando il protocollo TCP su porta 5005 al Raspberry.
\item Una volta ricevuti, il Raspberry li inoltrerà all'Arduino tramite il collegamento UART0 (Universal Asynchronous Receiver-Transmitter) che permette lo scambio su porta seriale in modo asincrono. Abbiamo deciso di rendere la comunicazione unidirezionale, solo il Pi invia Pin TX Pi - Pin Rx Arduino.  
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{img/uart0.png}
    \caption{Collegamento UART}
\end{figure}

\item \cite{flaskstream} Streaming Video Raspberry

    Abbiamo creato un'app Web Camera Server utilizzando il framework Flask e le librerie PiCamera2 e OpenCV per catturare i frame dalla PiCamera. L'app si occupa di inviare il flusso di frame catturato dalla camera al dispositivo connesso su porta 8080.
    Successivamente abbiamo reso lo script un servizio linux, in modo che venga eseguito in automatico all'accensione. Per la realizzazione dell'app ci siamo affidati alla seuente \href{https://github.com/miguelgrinberg/flask-video-streaming/tree/v1}{repository}. 
\end{enumerate}

\newpage

\subsection{Alimentazione}
\begin{itemize}
\item Arduino: Batteria 9v.
\item Motori: Batteria 9v.
\item Raspberry: Powerbank 5V/1A.
\end{itemize}

\section{Sviluppo Software}
\subsection{Introduzione}
Il programma di elaborazione dei frame sfrutta il \cite{multithreading} multithreading per ottenere prestazioni ottimali e fluide e si suddivide come segue:

\begin{itemize}
    \item Thread Principale: Esegue la Lane Daetection e invia i comandi relativi a direzione e ciò che Yolo ha individuato solo se il Thread1 ha acquisito il frame.
    \item Thread1: Si occupa di acquisire i frame. 
    \item Thread2: Esegue l'inferenza di Yolo e salva le classi rilevate.
\end{itemize} 

Di seguito è rappresentato il flusso di computazione semplificato:
\textit{Thread1 Frame Acquisito  $\rightarrow$ Thread2 Ogetti individuati nel frame $\rightarrow$ Thread Principale Lane Detection + Invio Comandi}  


\subsection{YoloV5s - Il Cervello}
    
    \subsubsection{Cosa è YoloV5}
    \cite{yolov5arch}
    YOLO (You Only Look Once) è una serie di modelli di riconoscimento degli oggetti in tempo reale basato sulla rete neurale convoluzionale. Il suo nome fa riferimento alla caratteristica che lo contraddistingue da modelli simili precedenti, ovvero che il modello richiede un solo passaggio di propagazione in avanti per poter fare delle previsioni.
    Il modello è stato creato da Joseph Redmon, che ha sviluppato le prime tre versioni e pubblicate sul suo sito web. Successive versioni sono state sviluppate da altri enti, la versione utilizzata da noi (v5) ad esempio è stata sviluppata dalla Ultralytics.
    
    \subsubsection{Architettura}
    \cite{yolotutorial}  \cite{yoloarch}
    Modelli di riconoscimento di oggetti classici richiedono due passaggi dell'immagine in input, la prima per generare un insieme di potenziali posizioni di oggetti (proposte), la seconda serve per raffinare tali proposte e ottenere le previsioni finali.
    Alcuni modelli, come il Region Proposal Networks, eseguono diverse istanze del modello per singola immagine, una per ogni regione separatamente. Tutto ciò richiede una quantità di risorse considerevole.
    Il modello YOLO esegue un singolo passaggio dell'immagine in input sia per le proposte e sia per la previsione finale, inoltre esegue le previsioni sull'immagine in input nella sua interezza nello stesso momento, grazie all'uso di un singolo layer completamente connesso. 
    Infine, la struttura stessa di YOLO è relativamente semplice: una singola rete convoluzionale esegue le previsioni sia delle classi sia dei bounding box. Questo è possibile grazie ad un approccio differente ed innovativo rispetto ad algoritmi precedenti, che sono essenzialmente dei modelli di classificazione modificati per la rilevazione di oggetti.
    La rete convoluzionale è suddivisa in tre parti: 
    \begin{itemize}
        \item Backbone: che è responsabile dell'estrazione delle caratteristiche dall'immagine di input.
        \item Neck: che esegue trasformazioni sulle caratteristiche estratte dalla Backbone
        \item Head: che si occupa delle previsioni finali.
    Versioni successive di YOLO sono aggiornamenti e modificazioni di questi tre moduli.
    \end{itemize}
    
\begin{figure}[h!]
    \centering
    \includegraphics[width=1.1\linewidth]{img/architetttura_yolo.jpg}
    \caption{Architettura YoloV5}
\end{figure}

\newpage
YOLO divide l'immagine in input in una griglia SxS. Se il centro di un oggetto ricade in una cella, è quella cella che si incarica di rilevarlo.
Ogni cella prevede B bounding box e un punteggio di confidenza per ognuno di quei box. La confidenza quantifica la probabilità di quel box di contenere un oggetto e con quale precisione. Inoltre ad ogni cella prevede C probabilità di classe, ovvero che tipo di oggetto si trova dentro la cella. 
Queste due informazioni vengono unite per produrre le previsioni finali.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{img/cf.jpg}
    \caption{Tiling Input}
\end{figure}

\subsubsection{Perchè lo abbiamo scelto}
Grazie a questo, YOLO è estremamente veloce, in grado di elaborare un flusso di immagini in tempo reale con bassissima latenza, pur mantenendo un alta precisione, e ciò è proprio il nostro caso di utilizzo: l'automobile deve riconoscere rapidamente i vari ostacoli e situazioni che si presentano lungo la strada, ed attuare strategie di correzione tempestivamente.

\subsubsection{Training}

  Il modello è stato addestrato su Google Colab.
  Abbiamo utilizzato un dataset creato sulla piattaforma Roboflow \cite{dataset}, composto da circa 3000 immagini con risoluzione 640x640, ed è stato aumentato ruotando immagini e aggiungendo rumore.
  
  Le classi con le relative annotazioni sono le seguenti:
  \begin{itemize}
    \item Pedoni: 1173
    \item Segnale di Stop: 261
    \item Segnale di limite a 50 km/h: 254
    \item Segnale di limite a 20 km/h: 423
    \item Semaforo rosso: 149
    \item Semaforo verde: 167
  \end{itemize}
  
  Iperparametri:
  \begin{itemize}
    \item 150 epoche
    \item Dimensione batch 64
    \item Learning rate 0.01
  \end{itemize}

  Questi iperparametri si sono rivelati ottimali per massimizzare le prestazioni del modello senza incorrere in overfitting. Ciò è stato compreso andando ad effettuare diverse sessioni di training con configurazioni differenti.
  Inoltre, abbiamo integrato il dataset con un maggior numero di immagini di segnali da 20 km/h, dopo aver riscontrato che il modello faticava a distinguerli da quelli da 50 km/h.
  I dati sono riferiti all'ultima istanza di allenamento del modello.

  Durante la fase di training viene generato un file CSV che contiene le metriche delle performance della rete neurale nella fase di validazione per ogni epoca del training.
  Nel caso di questo modello, e in generale per i modelli di object detection, ci sono diversi valori da osservare per comprendere il comportamento e l'efficacia della rete neurale.
  Precision P: Rappresenta il numero di previsioni corrette rispetto al numero di previsioni effettuate.
  Recall R: Rappresenta il numero di previsioni effettuate dal modello rispetto al numero effettivo di previsioni corrette presenti nell'immagine.
  Mean Average Precision mAP: Quantifica la performance generale del modello in rapporto tra i valori di P e R.
  Tutti i valori sono rappresentati come numeri decimali da 0 a 1.

  Intersection over Union (IoU): Rispetto alla classificazione, bisogna tenere conto anche di un altra capacità dei modelli di riconoscimento oggetti: quella di riconoscere la posizione di tali oggetti e delimitarli con una bounding box.
  Per valutare con precisione le prestazioni del modello si utilizza la metrica Intersection over Union (IoU), ovvero il rapporto tra l'intersezione dell'area della bounding box predetta rispetto all'area di quella effettiva, e l'unione di queste due aree.
  Più questo rapporto è vicino ad 1, più il bounding box predetto combacia con il bounding box corretto del dataset.
  

  \begin{figure}[h!]
    \centering
    \includegraphics[width=0.35\linewidth]{img/iou.png}
    \caption{Intersection over Union}
  \end{figure}
  
  \newpage

  Nel Mean Average Precision (mAP), viene impostato un threshold in cui una previsione con valore IoU superiore ad esso viene considerata corretta, e viene fatta una media di quante previsioni corrette esistono per tutte le immagini del dataset, per tutte le classi.
  Nel nostro caso abbiamo due metriche di mAP: una con threshold impostato a 0.5, e l'altra che rappresenta una media tra i valori di Average Precision con threshold a 0.5 e a 0.95, in modo da avere una metrica più severa.
  mAP@0.5 mostra un valore del 94,2\%, indicando un ottima precisione del modello. Normalmente il valore scende al 65\% per mAP@0.5:0.95, e si tratta comunque di un valore alto consideranto che il metro di giudizio di una previsione corretta è più stringente.

  Un valore alto di P significa che la maggior parte delle previsioni del modello sono corrette, mentre un valore basso significa un numero alto di falsi positivi.
  Un alto valore di R significa che il numero di previsioni corrette effettuate dal modello corrispondono al numero effettivo di positivi del dataset, un valore basso invece indica un alto numero di falsi negativi.
   Nel nostro caso, entrambi i valori superano il 90\% (P=93,1\%, R=90,9\%) mostrando che il modello si comporta bene nel riconoscere gli oggetti.

\begin{figure}[h!]
  \centering
  \captionsetup[subfigure]{labelformat=parens, labelsep=space}

  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/Training Box Loss.png}
    \caption{Box Loss}
    \label{fig:boxloss}
  \end{subfigure}
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/Training Class Loss.png}
    \caption{Class Loss}
    \label{fig:classloss}
  \end{subfigure}

  \vspace{0.6em}

  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/Training Object Loss.png}
    \caption{Object Loss}
    \label{fig:objectloss}
  \end{subfigure}
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/mAP_0.5.png}
    \caption{mAP\_0.5}
    \label{fig:map}
  \end{subfigure}

  \vspace{0.6em}

  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/Precision.png}
    \caption{Precision}
    \label{fig:precision}
  \end{subfigure}
  \begin{subfigure}[b]{0.495\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/Recall.png}
    \caption{Recall}
    \label{fig:recall}
  \end{subfigure}

  \caption{Andamento delle metriche durante l’addestramento del modello: perdite e performance.}
  \label{fig:training_metrics}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{img/confusion_matrix.png}
    \caption{Confusion Matrix.}
\end{figure}

    \subsubsection{Object Detection}
    Come descritto in precedenza, il modello identifica un elemento stradale per volta. In particolare per evitare confusioni con falsi positivi abbiamo deciso di salvare solo l'oggetto con maggiore confidenza. In futuro puntiamo a migliorare questo aspetto, in modo da poter salvare più oggetti contemporaneamente e prendere decisioni più complesse. Come ad esempio se il veicolo si trova davanti a un semaforo verde e un pedone, il veicolo dovrà fermarsi e dare la precedenza al pedone.
    Abbiamo utilizzato una formula chiamata \cite{distanza} Triangle Similarity per calcolare una stima della distanza tra il veicolo e l'oggetto individuato. Questa formula si basa sulla dimensione originale dell'oggetto in cm, la dimensione del bounding box in pixel e una distanza nota tra il veicolo e l'oggetto. In questo modo è possibile calcolare la distanza focale (La distanza tra il centro della lente (o l'obiettivo della fotocamera) e il punto focale, dove i raggi di luce paralleli convergono dopo essere passati attraverso la lente) e successivamente la distanza tra il veicolo e l'oggetto individuato.  
    \[
    Distanza focale = \frac{\text{Larghezza/Altezza (px)} \times \text{Distanza nota (cm)}}{\text{Larghezza/Altezza reale (cm)}}
    \]
    \[
    Distanza (cm) = \frac{\text{Larghezza/Altezza reale (cm)} \times \text{Distanza focale}}{\text{Larghezza/Altezza (px)}}
    \]

    Grazie a questo calcolo siamo in grado di stabilire una distanza ottimale per inviare il comando. Volendo essere più precisi conoscendo FOV e la larghezza del sensore in pixel abbiamo sostutuito il calcolo della distanza focale con la seguente formula \cite{focallenght}:
    \[
    Distanza focale = \frac{\text{Larghezza Sensore}}{2 \cdot \tan\left( \frac{\text{FOV in radianti}}{2} \right)}
    \]

    La lente della PiCamera ha un FOV (Field of View) di 62.2 gradi, non è quindi in grado di individuare oggetti a distanze molto ravvicinate. Un cartello al lato della strada già a circa 15 cm di distanza non è più nell'inquadratura. Per questo motivo è stato necessario inviare il comando preventivamente ed introdurre un delay in Arduino in modo da far fermare il veicolo (o cambiare la velocità) ad una distanza ottimale e realistica. 
    è quindi stato necessario inserire nel calcolo della distanza le dimensioni dei vari elementi della strada.    

\begin{figure}[h!]
    \centering
    \captionsetup[subfigure]{labelformat=parens,labelsep=space}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{img/pedone.png}
        \caption{Pedone} 
        \label{fig:pedone}
    \end{subfigure}

    \vspace{0.8em} 

    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/stop.png}
        \caption{Segnale Stop}
        \label{fig:stop}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/20.png}
        \caption{Segnale 20 km/h}
        \label{fig:20}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/50.png}
        \caption{Segnale 50 km/h} 
        \label{fig:50}
    \end{subfigure}

    \caption{Object Detection: esempi di classi individuate con accuratezza e stima della distanza.}
    \label{fig:object_detection}
\end{figure}



    Infine abbiamo notato che essendo un modello molto piccolo YoloV5s non è estremamente preciso, molto spesso per pochi frame la classe scompare per poi essere di nuovo individuata. Per questo motivo abbiamo introdotto un timer di tolleranza. Ad esempio se per un numero di frame consecutivi il modello non individua nulla o individua un'altra classe, non invierà alcun comando. Allo scadere del timer, verrà preso in considerazione ogni nuovo comando.
    

  
\newpage

\subsection{Lane Detection}
Il controllo della direzione viene calcolato prima dell'object detection nel thread principale. Abbiamo convertito in scala di grigi il frame e tracciato una linea orizzontale. Questa riga parte dal centro e si estende a tutto il lato destro (le Figure 9, 10, 11 sono croppate), per ogni pixel andiamo a rintracciare la striscia bianca in base all'intensità. Basandosi sulla posizione rilevata verrà stabilito se si dovrà effettuare la svolta o continuare dritto. Abbiamo impostato un range di pixel (rappresentato dalla linea bianca) che indica che il veicolo è posizionato correttamente e deve quindi procedere dritto. 

\textbf{Nota:} I comandi di direzione non sovrascrivono quelli dell'object detection. In questo modo una volta ferma allo stop l'analisi della corsia riprenderà solo se verrà dato il comando di ripartenza. 

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/centro.png}
        \label{fig:centro}
    \end{subfigure}
    \label{fig:posizione-centro}
    \caption{Veicolo Centrato, prosegue dritto.}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/destra.png}
        \label{fig:destra}
    \end{subfigure}
        \caption{Svolta a Destra}
    \label{fig:posizione-destra}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{img/sinistra.png}
        \label{fig:sinistra}
    \end{subfigure}
    \caption{Svolta a Sinistra}
    \label{fig:posizione-sinistra}
\end{figure}

\newpage

\section{Sviluppi futuri del progetto}
\begin{enumerate}
\item Migliorare il comparto Hardware sostituendo il Pi Z2W con un Pi 4/5 in combinazione con una camera con FOV maggiore. In questo modo la macchina sarà del tutto autonoma poichè il modello girerà direttamente sul Raspberry.
\item Implementare una lane detection più complessa:
    \begin{itemize}
    \item Prenda in considerazione entrambe le strisce di corsia.
    \item Calcoli un valore di intesità di svolta.
    \end{itemize}
\item Ottimizzare l'alimentazione in un'unica soluzione.
\end{enumerate}

\newpage

\medskip


\printbibliography


\end{document}



